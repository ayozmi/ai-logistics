{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai\n",
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import openai\n",
    "import sqlite3\n",
    "\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from webdriver_manager.chrome import ChromeDriverManager \n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the variables from .env into the environment\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to the News Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.maritimelogisticsprofessional.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install())) \n",
    "browser.implicitly_wait(5)\n",
    "browser.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Wait for the button to be clickable\n",
    "    WebDriverWait(browser, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"//button[text()='Ok']\"))\n",
    "    )\n",
    "    # Click the button once it is clickable\n",
    "    browser.find_element(By.XPATH, \"//button[text()='Ok']\").click()\n",
    "except (NoSuchElementException, TimeoutException):\n",
    "    print(\"The cookie acceptance button was not found on the page.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of latest news\n",
    "latest_news = browser.find_elements(By.CLASS_NAME, \"snippet-flex\")\n",
    "latest_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all URLs before navigating\n",
    "article_urls = [element.get_attribute('href') for element in latest_news if '/news/' in element.get_attribute('href')]\n",
    "# Premium version\n",
    "premium = False\n",
    "\n",
    "article_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Article\n",
    "article_link = latest_news[0].get_attribute('href')\n",
    "article_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get article\n",
    "time.sleep(5)\n",
    "browser.get(article_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the title element and get the text\n",
    "title_element = browser.find_element(By.CSS_SELECTOR, \"h1[itemprop='name']\")\n",
    "article_title = title_element.text\n",
    "\n",
    "# Find the article body element and get all the paragraph texts\n",
    "article_body_element = browser.find_element(By.CSS_SELECTOR, \"div[property='articleBody']\")\n",
    "article_paragraphs = article_body_element.find_elements(By.TAG_NAME, \"p\")\n",
    "\n",
    "# Combine the text of all paragraphs to form the body text\n",
    "article_text = \" \".join(paragraph.text for paragraph in article_paragraphs)\n",
    "\n",
    "# Now you have the article's title and text\n",
    "print(\"Title:\", article_title)\n",
    "print(\"Article Text:\", article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all URLs before navigating\n",
    "urls = [element.get_attribute('href') for element in latest_news]\n",
    "\n",
    "for url in urls:\n",
    "    print(f\"Getting {url}\")\n",
    "    time.sleep(5)  # Consider using WebDriverWait here instead of time.sleep for better efficiency\n",
    "    browser.get(url)\n",
    "    # Now you can perform your scraping logic on each article page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorizing the Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load keywords from JSON file\n",
    "with open('../json/keywords.json', 'r') as file:\n",
    "    keywords = json.load(file)\n",
    "\n",
    "def classify_article(article_text, keywords):\n",
    "    max_count = 0\n",
    "    max_category = \"Unclassified or Neutral\"\n",
    "\n",
    "    # Convert article text to lower case for comparison\n",
    "    article_text_lower = article_text.lower()\n",
    "    \n",
    "    # Check for the presence of each keyword in the article\n",
    "    for category, category_keywords in keywords.items():\n",
    "        count = sum(keyword in article_text_lower for keyword in category_keywords)\n",
    "        if count > max_count:\n",
    "            max_count = count\n",
    "            max_category = category\n",
    "\n",
    "    return max_category\n",
    "\n",
    "# Usage example:\n",
    "classification = classify_article(article_text, keywords)\n",
    "print(f\"The article is related to: {classification}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "def summarize_text(text):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt= f\"\"\"\n",
    "Please summarize the key points of the article for a business audience in a concise paragraph, limiting the summary to no more than 350 characters. Then, in a separate paragraph of 500 characters, elaborate on the potential impact of the situation described in the article on maritime logistics, port operations, and supply chain management, specifically focusing on its implications for Latin America. Label this second paragraph 'Impacto en LATAM:' and ensure there is a clear separation between the two sections. Present your response in Spanish and format it as markdown text for clarity:\\n\\n{text}\n",
    "                \"\"\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=150,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "#summary = summarize_text(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = \"Get Premium for enabling AI-powered summary!\"\n",
    "location = \"Global / LATAM\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Original Article, Title and Summary in DDBB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establishes a connection to the SQLite database\n",
    "def connect_to_db(db_path):\n",
    "    return sqlite3.connect(db_path)\n",
    "\n",
    "# Create a new table for storing only daily articles\n",
    "def create_daily_table(cursor, table_name):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    cursor.execute(f\"\"\"CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                        title TEXT,\n",
    "                        text TEXT,\n",
    "                        summary TEXT,\n",
    "                        classification TEXT,\n",
    "                        location TEXT\n",
    "                    );\"\"\")\n",
    "\n",
    "# Create a new table for storing daily links\n",
    "def create_daily_links_table(cursor, table_name, articles_table_name):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    cursor.execute(f\"\"\"CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                        article_id INTEGER,\n",
    "                        title TEXT,\n",
    "                        link TEXT,\n",
    "                        FOREIGN KEY (article_id) REFERENCES {articles_table_name}(id)\n",
    "                    );\"\"\")\n",
    "\n",
    "# Inserts an article into the daily table\n",
    "def insert_article_data(cursor, table_name, article_title, article_text, summary, classification, location):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Check if an article with the same title already exists\n",
    "    cursor.execute(f\"SELECT id FROM {table_name} WHERE title = ?\", (article_title,))\n",
    "    existing_article = cursor.fetchone()\n",
    "\n",
    "    if existing_article == None:\n",
    "        cursor.execute(f\"\"\"INSERT INTO {table_name} (title, text, summary, classification, location)\n",
    "                        VALUES (?, ?, ?, ?, ?);\"\"\",\n",
    "                    (article_title, article_text, summary, classification, location))\n",
    "        return cursor.lastrowid  # Return the ID of the new article\n",
    "    else:\n",
    "        return existing_article[0]  # Return the ID of the existing article\n",
    "\n",
    "# Inserts a link into the daily links table\n",
    "def insert_link_data(cursor, table_name, article_id, article_title, link):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Check if the link already exists for the given article\n",
    "    cursor.execute(f\"SELECT id FROM {table_name} WHERE article_id = ? AND link = ?\", (article_id, link))\n",
    "    existing_link = cursor.fetchone()\n",
    "\n",
    "    if existing_link is None:\n",
    "        # If the link does not exist for this article, insert it\n",
    "        cursor.execute(f\"\"\"INSERT INTO {table_name} (article_id, title, link)\n",
    "                           VALUES (?, ?, ?);\"\"\",\n",
    "                       (article_id, article_title, link))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDBB Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the SQLite database\n",
    "db_path = '../data/news/maritime_news.db'\n",
    "\n",
    "# Define table naming\n",
    "current_date = datetime.now().strftime(\"%m%d%Y\")\n",
    "\n",
    "# Establish a connection and create a cursor\n",
    "conn = connect_to_db(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create tables for today's date with news\n",
    "news_table_name = f\"news_{current_date}\"\n",
    "create_daily_table(cursor, news_table_name)\n",
    "\n",
    "# Create table for storing links\n",
    "links_table_name = f\"news_links_{current_date}\"\n",
    "create_daily_links_table(cursor, links_table_name, news_table_name)\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDBB News/Links Input or Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a connection and create a cursor\n",
    "conn = connect_to_db(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Insert the article data into the table and get the article ID\n",
    "create_daily_table(cursor, news_table_name)\n",
    "article_id = insert_article_data(cursor, news_table_name, article_title, article_text, summary, classification, location)\n",
    "\n",
    "# Insert the link data into the links table\n",
    "insert_link_data(cursor, links_table_name, article_id, article_title, article_link)\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a browser session\n",
    "browser = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install())) \n",
    "browser.implicitly_wait(2)\n",
    "browser.get(URL)\n",
    "\n",
    "# Cookies button\n",
    "try:\n",
    "    # Wait for the button to be clickable\n",
    "    WebDriverWait(browser, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"//button[text()='Ok']\"))\n",
    "    )\n",
    "    # Click the button once it is clickable\n",
    "    browser.find_element(By.XPATH, \"//button[text()='Ok']\").click()\n",
    "except (NoSuchElementException, TimeoutException):\n",
    "    print(\"The cookie acceptance button was not found on the page.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GLOBAL LATEST NEWS\n",
    "\"\"\"\n",
    "\n",
    "# List of latest global news\n",
    "latest_news = browser.find_elements(By.CLASS_NAME, \"snippet-flex\")\n",
    "# Collect all URLs before navigating\n",
    "article_urls = [element.get_attribute('href') for element in latest_news]\n",
    "# Premium version\n",
    "premium = False\n",
    "\n",
    "# Establish a connection and create a cursor\n",
    "conn = connect_to_db(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "for article in article_urls:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Get article\n",
    "    time.sleep(5)\n",
    "    browser.get(article)\n",
    "\n",
    "    # Find the title element and get the text\n",
    "    title_element = browser.find_element(By.CSS_SELECTOR, \"h1[itemprop='name']\")\n",
    "    article_title = title_element.text\n",
    "\n",
    "    # Find the article body element and get all the paragraph texts\n",
    "    article_body_element = browser.find_element(By.CSS_SELECTOR, \"div[property='articleBody']\")\n",
    "    article_paragraphs = article_body_element.find_elements(By.TAG_NAME, \"p\")\n",
    "\n",
    "    # Combine the text of all paragraphs to form the body text\n",
    "    article_text = \" \".join(paragraph.text for paragraph in article_paragraphs)\n",
    "\n",
    "    # AI-powered Summary\n",
    "    if premium:\n",
    "        summarize_text(article_text)\n",
    "    else:\n",
    "        # Summary not premium\n",
    "        summary = \"Get Premium for enabling AI-powered summary!\"\n",
    "    \n",
    "    # Article classification\n",
    "    classification = classify_article(article_text, keywords)\n",
    "    # Article location\n",
    "    location = \"Global\"\n",
    "    \n",
    "    # Insert the article data into the table and get the article ID\n",
    "    article_id = insert_article_data(cursor, news_table_name, article_title, article_text, summary, classification, location)\n",
    "    # Insert the link data into the links table\n",
    "    insert_link_data(cursor, links_table_name, article_id, article_title, article_link)\n",
    "\n",
    "    # Commit the changes\n",
    "    conn.commit()\n",
    "\n",
    "# Close browser and database connection\n",
    "browser.quit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.maritimelogisticsprofessional.com\"\n",
    "\n",
    "# Create a browser session\n",
    "browser = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install())) \n",
    "browser.get(URL)\n",
    "\n",
    "# Define a wait variable with a timeout of 10 seconds\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "# Cookies button\n",
    "try:\n",
    "    # Wait for the button to be clickable and click it\n",
    "    cookie_button = wait.until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"//button[text()='Ok']\")),\n",
    "        message=\"Cookie button not clickable.\"\n",
    "    )\n",
    "    cookie_button.click()\n",
    "except TimeoutException:\n",
    "    print(\"The cookie acceptance button was not found on the page.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for LATAM\n",
    "filter_news = '/south-america'\n",
    "# Wait for the category links to be present and iterate through them\n",
    "cat_links = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".cat-link\")))\n",
    "\n",
    "latam_link = None\n",
    "for cat_link_div in cat_links:\n",
    "    a_element = cat_link_div.find_element(By.TAG_NAME, 'a')\n",
    "    href = a_element.get_attribute('href')\n",
    "    if href.endswith(filter_news):\n",
    "        latam_link = href\n",
    "        break  # Exit the loop once we find the LATAM link\n",
    "\n",
    "# Check if the LATAM link was found before proceeding\n",
    "if latam_link:\n",
    "    # Now navigate to the LATAM link\n",
    "    browser.get(latam_link)\n",
    "\n",
    "    # Wait for snippets to be present after navigating to the LATAM page\n",
    "    snippets = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".snippet\")))\n",
    "\n",
    "    # Process each snippet\n",
    "    for snippet in snippets:\n",
    "        news_link = snippet.get_attribute('href')\n",
    "        news_title = snippet.find_element(By.TAG_NAME, 'h2').text\n",
    "        body_text = snippet.find_element(By.TAG_NAME, 'p').text\n",
    "\n",
    "        print(news_link)\n",
    "        print(news_title)\n",
    "        print(body_text)\n",
    "else:\n",
    "    print(\"LATAM link was not found\")\n",
    "\n",
    "browser.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
