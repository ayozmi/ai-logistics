{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5_Modeling: Shipping Cost Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Process\n",
    "\n",
    "- Explore 4 regression models, 1 as baseline and the others more advanced\n",
    "- Train test splits\n",
    "- Apply cross validation and hyperparamter tunning in at least 2 models\n",
    "- Get the feature importance and see which feature is more relevant\n",
    "- Apply cross validation for evaluation\n",
    "- Draw a table comparing the performances of each model. Example:\n",
    "\n",
    "# Model Comparison\n",
    "\n",
    "In this section, we compare the performance of different models using various metrics. The models evaluated include [Model 1], [Model 2], and [Model 3]. The following metrics are used for comparison:\n",
    "\n",
    "- **Accuracy**: The ratio of correctly predicted observations to the total observations. It is a useful metric when the classes are well balanced.\n",
    "- **Precision**: The ratio of correctly predicted positive observations to the total predicted positives. High precision relates to the low false positive rate.\n",
    "- **Recall (Sensitivity)**: The ratio of correctly predicted positive observations to all observations in the actual class. High recall relates to the low false negative rate.\n",
    "- **F1 Score**: The weighted average of Precision and Recall. It is a better metric than accuracy for imbalanced datasets.\n",
    "- **ROC-AUC Score**: Area Under the Receiver Operating Characteristic Curve, which is a performance measurement for classification problems at various threshold settings.\n",
    "\n",
    "# Modeling Output\n",
    "\n",
    "The output of this stage will be pickle files for future integration with the application. We will save all tested models and pick the best performer for the application integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import pickle\n",
    "import os\n",
    "import boto3\n",
    "from io import StringIO, BytesIO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore data conversion warnings\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data from Feat Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for .env file at: c:\\repos\\ai-logistics\\notebooks\\ShipCostPredictor\\..\\../.env\n",
      "Loaded BUCKET_NAME_INBOUND=logimo-inbound\n",
      "Loaded BUCKET_NAME_ALIGNED=logimo-aligned\n",
      "Loaded BUCKET_NAME_OUTBOUND=logimo-outbound\n",
      "Loaded PREFIX_KEY=ship_cost_predictor\n"
     ]
    }
   ],
   "source": [
    "# Load variables from .env file, ignoring lines without '='\n",
    "def load_env_variables(env_file='../.env'):\n",
    "    # Get the current working directory\n",
    "    current_dir = os.getcwd()\n",
    "    env_path = os.path.join(current_dir, '..', env_file)\n",
    "    \n",
    "    print(f\"Looking for .env file at: {env_path}\")  # Debugging output\n",
    "\n",
    "    if not os.path.exists(env_path):\n",
    "        print(f\".env file does not exist at: {env_path}\")\n",
    "        return\n",
    "\n",
    "    with open(env_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Skip lines without an equals sign or comments\n",
    "            if '=' in line and not line.strip().startswith('#'):\n",
    "                key, value = line.strip().split('=', 1)\n",
    "                os.environ[key] = value\n",
    "                print(f\"Loaded {key}={value}\")  # Debugging output\n",
    "\n",
    "# Load environment variables\n",
    "load_env_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Specify the bucket name and prefix (folder path)\n",
    "bucket_name = os.getenv('BUCKET_NAME_ALIGNED')\n",
    "prefix = os.getenv('PREFIX_KEY')\n",
    "\n",
    "# Fetch the content of the cleaned CSV file from S3\n",
    "obj = s3.get_object(Bucket=bucket_name, \n",
    "                    Key=f'{prefix}/regression_model_X_pca.csv')\n",
    "\n",
    "# Read the content of the CSV file\n",
    "csv_content = obj['Body'].read().decode('utf-8')\n",
    "\n",
    "# Use pandas to read the CSV content into a DataFrame\n",
    "X_pca = pd.read_csv(StringIO(csv_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Specify the bucket name and prefix (folder path)\n",
    "bucket_name = os.getenv('BUCKET_NAME_ALIGNED')\n",
    "prefix = os.getenv('PREFIX_KEY')\n",
    "\n",
    "# Fetch the content of the cleaned CSV file from S3\n",
    "obj = s3.get_object(Bucket=bucket_name, \n",
    "                    Key=f'{prefix}/regression_model_Y_pca.csv')\n",
    "\n",
    "# Read the content of the CSV file\n",
    "csv_content = obj['Body'].read().decode('utf-8')\n",
    "\n",
    "# Use pandas to read the CSV content into a DataFrame\n",
    "y = pd.read_csv(StringIO(csv_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_pca=pd.read_csv('../../data/shipcost_predictor/outbound/supply_chain_X_pca.csv')\n",
    "#y=pd.read_csv('../../data/shipcost_predictor/outbound/supply_chain_Y_pca.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression RMSE: 30.034397689991362\n",
      "Linear Regression R2: -0.030678180531177768\n"
     ]
    }
   ],
   "source": [
    "lr_model = LinearRegression()\n",
    "\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "lr_mse = mean_squared_error(y_test, y_pred_lr)\n",
    "lr_rmse = np.sqrt(lr_mse)\n",
    "lr_r2 = r2_score(y_test, y_pred_lr)\n",
    "\n",
    "print(f'Linear Regression RMSE: {lr_rmse}')\n",
    "print(f'Linear Regression R2: {lr_r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Linear Regression\n",
      "Mean Squared Error: 902.0650446005585\n",
      "Root Mean Squared Error: 30.034397689991362\n",
      "R^2 Score: -0.030678180531177768\n",
      "\n",
      "Model: Random Forest\n",
      "Mean Squared Error: 893.4643285847767\n",
      "Root Mean Squared Error: 29.890873667137544\n",
      "R^2 Score: -0.02085120587178757\n",
      "\n",
      "Model: SVR\n",
      "Mean Squared Error: 877.3263591929172\n",
      "Root Mean Squared Error: 29.619695460840195\n",
      "R^2 Score: -0.002412343807650208\n",
      "\n",
      "Model: K-Nearest Neighbors\n",
      "Mean Squared Error: 1001.7053044245808\n",
      "Root Mean Squared Error: 31.64972834677386\n",
      "R^2 Score: -0.14452478429637772\n",
      "\n",
      "Model: Gradient Boosting\n",
      "Mean Squared Error: 905.0747560094007\n",
      "Root Mean Squared Error: 30.084460374243058\n",
      "R^2 Score: -0.03411700558859154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Models to try for the regression\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(),\n",
    "    'SVR': SVR(),\n",
    "    'K-Nearest Neighbors': KNeighborsRegressor(),\n",
    "    'Gradient Boosting': GradientBoostingRegressor()\n",
    "}\n",
    "\n",
    "# Results of all models and saving\n",
    "results = {}\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)  # Calculate RMSE\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    results[model_name] = {\n",
    "        'Mean Squared Error': mse,\n",
    "        'Root Mean Squared Error': rmse,\n",
    "        'R^2 Score': r2\n",
    "    }\n",
    "    \n",
    "    # Save the model as a pickle file\n",
    "    with open(f\"../../models/price_predictor/{model_name.replace(' ', '_').lower()}_model.pkl\", 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Mean Squared Error: {metrics['Mean Squared Error']}\")\n",
    "    print(f\"Root Mean Squared Error: {metrics['Root Mean Squared Error']}\")\n",
    "    print(f\"R^2 Score: {metrics['R^2 Score']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's your filled-out table:\n",
    "\n",
    "## Model Metrics\n",
    "\n",
    "| **Model**            | **MSE** | **RMSE** | **R^2 Score** | \n",
    "|----------------------|---------|----------|---------------|\n",
    "| **Linear Regression**| 902.07  | 30.03    | -0.03         |\n",
    "| **Random Forest**    | 909.21  | 30.15    | -0.04         | \n",
    "| **SVR**              | 877.33  | 29.62    | -0.002        | \n",
    "| **KNN**              | 1001.71 | 31.65    | -0.14         |\n",
    "| **Gradient Boosting**| 906.22  | 30.10    | -0.04         |\n",
    "\n",
    "### Interpretation of Results and Conclusion:\n",
    "\n",
    "Looking at the results, all models seem to perform poorly in predicting shipping prices for the supply chain application. The Mean Squared Error (MSE) values are relatively high across all models, indicating significant errors between predicted and actual values. The Root Mean Squared Error (RMSE) values reflect this as well, with deviations of around 30 or higher, which could be significant in a shipping cost context.\n",
    "\n",
    "Furthermore, the R^2 scores are all negative, indicating that the models are performing worse than a simple horizontal line at the mean of the data would. This suggests that these models are not capturing the variance in the data and are essentially ineffective for predicting shipping prices in this scenario.\n",
    "\n",
    "Despite using a variety of algorithms, including Linear Regression, Random Forest, Support Vector Regression (SVR), K-Nearest Neighbors (KNN), and Gradient Boosting, the prediction made on the prices is 30 units off the real price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible next steps could involve:\n",
    "1. **Feature Engineering**: Refining or adding features that might better capture the nuances of shipping costs.\n",
    "2. **Model Tuning**: Adjusting hyperparameters or trying different algorithms that might perform better on this specific problem.\n",
    "3. **Data Collection**: Ensuring that the dataset adequately represents the factors influencing shipping prices.\n",
    "4. **Domain Expertise**: Consulting with domain experts to better understand the factors at play and refine the modeling approach accordingly.\n",
    "\n",
    "In conclusion, further exploration and refinement are needed to develop a more accurate predictive model for shipping prices in this supply chain context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File logistic_regression_model.pkl saved to s3://logimo-outbound/ship_cost_predictor/classification_models_files/logistic_regression_model.pkl\n",
      "File neural_network_model.h5 saved to s3://logimo-outbound/ship_cost_predictor/classification_models_files/neural_network_model.h5\n",
      "File random_forest_model.pkl saved to s3://logimo-outbound/ship_cost_predictor/classification_models_files/random_forest_model.pkl\n",
      "File scaler.pkl saved to s3://logimo-outbound/ship_cost_predictor/classification_models_files/scaler.pkl\n",
      "File svm_model.pkl saved to s3://logimo-outbound/ship_cost_predictor/classification_models_files/svm_model.pkl\n",
      "All files have been uploaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Uploading models to S3 bucket for classification\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Specify the directory containing the files\n",
    "directory_path = '../../models/price_predictor/classification'\n",
    "output_bucket_name = 'logimo-outbound' \n",
    "prefix = 'ship_cost_predictor/classification_models_files'\n",
    "\n",
    "# Iterate over all files in the directory\n",
    "for filename in os.listdir(directory_path):\n",
    "    file_path = os.path.join(directory_path, filename)\n",
    "    \n",
    "    # Ensure it's a file (not a directory)\n",
    "    if os.path.isfile(file_path):\n",
    "        # Read the file in binary mode\n",
    "        with open(file_path, 'rb') as f:\n",
    "            file_data = f.read()\n",
    "        \n",
    "        # Convert file data to a binary stream\n",
    "        file_buffer = BytesIO(file_data)\n",
    "        \n",
    "        # Create the S3 key (path) for the file\n",
    "        output_prefix = f'{prefix}/{filename}'\n",
    "        \n",
    "        # Upload the file to S3\n",
    "        s3.put_object(Bucket=output_bucket_name, Key=output_prefix, Body=file_buffer.getvalue())\n",
    "        \n",
    "        print(f\"File {filename} saved to s3://{output_bucket_name}/{output_prefix}\")\n",
    "\n",
    "print(\"All files have been uploaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sqlalchemy\n",
      "  Downloading SQLAlchemy-2.0.30-cp311-cp311-win_amd64.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\pablo\\.conda\\envs\\python-dsf\\lib\\site-packages (from sqlalchemy) (4.9.0)\n",
      "Collecting greenlet!=0.4.17 (from sqlalchemy)\n",
      "  Downloading greenlet-3.0.3-cp311-cp311-win_amd64.whl.metadata (3.9 kB)\n",
      "Downloading SQLAlchemy-2.0.30-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/2.1 MB 1.1 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.4/2.1 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.9/2.1 MB 5.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.7/2.1 MB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 8.3 MB/s eta 0:00:00\n",
      "Downloading greenlet-3.0.3-cp311-cp311-win_amd64.whl (292 kB)\n",
      "   ---------------------------------------- 0.0/292.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 292.8/292.8 kB 18.8 MB/s eta 0:00:00\n",
      "Installing collected packages: greenlet, sqlalchemy\n",
      "Successfully installed greenlet-3.0.3 sqlalchemy-2.0.30\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymysql\n",
      "  Downloading PyMySQL-1.1.1-py3-none-any.whl.metadata (4.4 kB)\n",
      "Downloading PyMySQL-1.1.1-py3-none-any.whl (44 kB)\n",
      "   ---------------------------------------- 0.0/45.0 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 10.2/45.0 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 41.0/45.0 kB 495.5 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 45.0/45.0 kB 373.5 kB/s eta 0:00:00\n",
      "Installing collected packages: pymysql\n",
      "Successfully installed pymysql-1.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for .env file at: c:\\repos\\ai-logistics\\notebooks\\ShipCostPredictor\\..\\../.env\n",
      "Loaded BUCKET_NAME_INBOUND=logimo-inbound\n",
      "Loaded BUCKET_NAME_ALIGNED=logimo-aligned\n",
      "Loaded BUCKET_NAME_OUTBOUND=logimo-outbound\n",
      "Loaded PREFIX_KEY=ship_cost_predictor\n"
     ]
    }
   ],
   "source": [
    "def load_env_variables(env_file='../.env'):\n",
    "    # Get the current working directory\n",
    "    current_dir = os.getcwd()\n",
    "    env_path = os.path.join(current_dir, '..', env_file)\n",
    "    \n",
    "    print(f\"Looking for .env file at: {env_path}\")  # Debugging output\n",
    "\n",
    "    if not os.path.exists(env_path):\n",
    "        print(f\".env file does not exist at: {env_path}\")\n",
    "        return\n",
    "\n",
    "    with open(env_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Skip lines without an equals sign or comments\n",
    "            if '=' in line and not line.strip().startswith('#'):\n",
    "                key, value = line.strip().split('=', 1)\n",
    "                os.environ[key] = value\n",
    "                print(f\"Loaded {key}={value}\")  # Debugging output\n",
    "\n",
    "# Load environment variables\n",
    "load_env_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS credentials and settings from environment variables\n",
    "aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "region_name = os.getenv('AWS_REGION')\n",
    "csv_bucket_name = os.getenv('BUCKET_NAME_INBOUND')\n",
    "csv_file_key = 'path/to/your/csvfile.csv'\n",
    "model_bucket_name = os.getenv('BUCKET_NAME_ALIGNED')\n",
    "model_file_key = 'path/to/your/model.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MySQL database settings from environment variables\n",
    "rds_host = os.getenv('RDS_HOST')\n",
    "rds_port = os.getenv('RDS_PORT')\n",
    "rds_dbname = os.getenv('RDS_DBNAME')\n",
    "rds_user = os.getenv('RDS_USER')\n",
    "rds_password = os.getenv('RDS_PASSWORD')\n",
    "table_name = os.getenv('TABLE_NAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize boto3 client\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    region_name=region_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the bucket name and prefix (folder path)\n",
    "bucket_name = os.getenv('BUCKET_NAME_INBOUND')\n",
    "prefix = os.getenv('PREFIX_KEY')\n",
    "\n",
    "# List objects in the specified S3 folder\n",
    "response = s3.list_objects_v2(Bucket=bucket_name, Prefix=f'{prefix}/data-ingest')\n",
    "\n",
    "\n",
    "# Initialize a list to store CSV file keys\n",
    "csv_files = []\n",
    "\n",
    "# Iterate over the objects and collect keys of CSV files\n",
    "for obj in response.get('Contents', []):\n",
    "    object_key = obj['Key']\n",
    "    if object_key.endswith('.csv'):\n",
    "        csv_files.append(object_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV file from S3\n",
    "csv_obj = s3.get_object(Bucket=csv_bucket_name, Key=csv_file_key)\n",
    "csv_data = csv_obj['Body']\n",
    "df = pd.read_csv(csv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from S3\n",
    "model_obj = s3.get_object(Bucket=model_bucket_name, Key=model_file_key)\n",
    "model_body = model_obj['Body'].read()\n",
    "model = pickle.loads(model_body)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(df)\n",
    "df['prediction'] = predictions\n",
    "\n",
    "# Create SQLAlchemy engine for MySQL connection\n",
    "connection_string = f'mysql+pymysql://{rds_user}:{rds_password}@{rds_host}:{rds_port}/{rds_dbname}'\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Write to MySQL database\n",
    "df.to_sql(name=table_name, con=engine, if_exists='append', index=False)\n",
    "\n",
    "print('Data successfully written to the RDS MySQL instance.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDS_HOST=your-rds-endpoint\n",
    "RDS_PORT=3306\n",
    "RDS_DBNAME=your-db-name\n",
    "RDS_USER=your-username\n",
    "RDS_PASSWORD=your-password\n",
    "TABLE_NAME=your-table-name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
