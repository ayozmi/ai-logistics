{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO-DO: \n",
    "\n",
    "- Explore 4 regression models, 1 as baseline and the others more advanced\n",
    "- Train test splits\n",
    "- Apply cross validation and hyperparamter tunning in at least 2 models\n",
    "- Get the feature importance and see which feature is more relevant\n",
    "- Apply cross validation for evaluation\n",
    "- Draw a table comparing the performances of each model. Example:\n",
    "\n",
    "# Model Comparison\n",
    "\n",
    "In this section, we compare the performance of different models using various metrics. The models evaluated include [Model 1], [Model 2], and [Model 3]. The following metrics are used for comparison:\n",
    "\n",
    "- **Accuracy**: The ratio of correctly predicted observations to the total observations. It is a useful metric when the classes are well balanced.\n",
    "- **Precision**: The ratio of correctly predicted positive observations to the total predicted positives. High precision relates to the low false positive rate.\n",
    "- **Recall (Sensitivity)**: The ratio of correctly predicted positive observations to all observations in the actual class. High recall relates to the low false negative rate.\n",
    "- **F1 Score**: The weighted average of Precision and Recall. It is a better metric than accuracy for imbalanced datasets.\n",
    "- **ROC-AUC Score**: Area Under the Receiver Operating Characteristic Curve, which is a performance measurement for classification problems at various threshold settings.\n",
    "\n",
    "## Model Metrics\n",
    "\n",
    "| **Model**       | **Accuracy** | **Precision** | **Recall** | **F1 Score** | **ROC-AUC Score** |\n",
    "|-----------------|--------------|---------------|------------|--------------|-------------------|\n",
    "| **Model 1**     | 0.XX         | 0.XX          | 0.XX       | 0.XX         | 0.XX              |\n",
    "| **Model 2**     | 0.XX         | 0.XX          | 0.XX       | 0.XX         | 0.XX              |\n",
    "| **Model 3**     | 0.XX         | 0.XX          | 0.XX       | 0.XX         | 0.XX              |\n",
    "\n",
    "## Interpretation of Results\n",
    "\n",
    "- **Model 1**: [Brief summary of performance]\n",
    "- **Model 2**: [Brief summary of performance]\n",
    "- **Model 3**: [Brief summary of performance]\n",
    "\n",
    "Based on the comparison, we can observe that [Model X] performs the best overall due to its higher [Metric(s)]. However, depending on the specific use case and requirements (e.g., higher precision vs. recall), [Model Y] might be preferable.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The chosen model for deployment is [Model Z] due to its superior performance in [specific metric(s)]. Further fine-tuning and validation will be conducted to ensure robustness and reliability in a real-world scenario.\n",
    "\n",
    "\n",
    "**OUTPUT**: The best performer model saved in a pickle file in '..\\..\\repos\\ai-logistics\\models\\price_predictor'"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
